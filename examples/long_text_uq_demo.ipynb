{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Long-Text Uncertainty Quantification\n",
    "\n",
    "<div style=\"background-color: rgba(200, 200, 200, 0.1); padding: 20px; border-radius: 8px; margin-bottom: 20px; border: 1px solid rgba(127, 127, 127, 0.2); max-width: 97.5%; overflow-wrap: break-word;\">\n",
    "  <p style=\"font-size: 16px; line-height: 1.6\">\n",
    "    Long-Text Uncertainty Quantification (LUQ) is a long-form adaptation of black-box uncertainty quantification. This approach generates multiple responses to the same prompt, decomposes those responses into granular units (sentences or claims), and scores those units by measuring whether sampled responses entail each unit. This demo provides an illustration \n",
    "    of how to use the LUQ methods with <code>uqlm</code>. The available scorers and papers from which they are adapted are below:\n",
    "  </p>\n",
    "      \n",
    "*   Long-text Uncertainty Quantification (LUQ) ([Zhang et al., 2024](https://arxiv.org/abs/2403.20279))\n",
    "*   LUQ-Atomic ([Zhang et al., 2024](https://arxiv.org/abs/2403.20279))\n",
    "*   LUQ-pair ([Zhang et al., 2024](https://arxiv.org/abs/2403.20279))\n",
    "*   Generalized LUQ-pair ([Zhang et al., 2024](https://arxiv.org/abs/2403.20279))\n",
    "\n",
    "</div>\n",
    "\n",
    "## üìä What You'll Do in This Demo\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>1</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section1>Set up LLM and prompts.</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Set up LLM instance and load example data prompts.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>2</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section2>Generate LLM Responses and Confidence Scores</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Generate responses and compute claim-level confidence scores using the <code>LongTextUQ()</code> class.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 25px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>3</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section3>Evaluate Hallucination Detection Performance</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Grade claims with `FactScoreGrader` class and evaluate claim-level hallucination detection.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "## ‚öñÔ∏è Advantages & Limitations\n",
    "\n",
    "<div style=\"display: flex; gap: 20px\">\n",
    "  <div style=\"flex: 1; background-color: rgba(0, 200, 0, 0.1); padding: 15px; border-radius: 8px; border: 1px solid rgba(0, 200, 0, 0.2)\">\n",
    "    <h3 style=\"color: #2e8b57; margin-top: 0\">Pros</h3>\n",
    "    <ul style=\"margin-bottom: 0\">\n",
    "      <li><strong>Universal Compatibility:</strong> Works with any LLM without requiring token probability access</li>\n",
    "      <li><strong>Fine-Grained Scoring:</strong> Score at sentence or claim-level to localize likely hallucinations</li>\n",
    "      <li><strong>Uncertainty-aware decoding:</strong> Improve factual precision by dropping high-uncertainty claims</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"flex: 1; background-color: rgba(200, 0, 0, 0.1); padding: 15px; border-radius: 8px; border: 1px solid rgba(200, 0, 0, 0.2)\">\n",
    "    <h3 style=\"color: #b22222; margin-top: 0\">Cons</h3>\n",
    "    <ul style=\"margin-bottom: 0\">\n",
    "      <li><strong>Higher Cost:</strong> Requires multiple generations per prompt</li>\n",
    "      <li><strong>Slower:</strong> Multiple generations and comparison calculations increase latency</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from uqlm import LongTextUQ\n",
    "from uqlm.utils import claims_dicts_to_lists, load_example_dataset, plot_model_accuracies\n",
    "from uqlm.longform import FactScoreGrader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Set up LLM and Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will illustrate this approach using the [FactScore](https://github.com/shmsw25/FActScore/tree/main/factscore) longform QA dataset. To implement with your use case, simply **replace the example prompts with your data**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset - factscore...\n",
      "Processing dataset...\n",
      "Dataset ready!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>wikipedia_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me a bio of Suthida within 100 words.\\n</td>\n",
       "      <td>Suthida Bajrasudhabimalalakshana (Thai: ‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tell me a bio of Miguel √Ångel F√©lix Gallardo w...</td>\n",
       "      <td>Miguel √Ångel F√©lix Gallardo (born January 8, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0       Tell me a bio of Suthida within 100 words.\\n   \n",
       "1  Tell me a bio of Miguel √Ångel F√©lix Gallardo w...   \n",
       "\n",
       "                                      wikipedia_text  \n",
       "0  Suthida Bajrasudhabimalalakshana (Thai: ‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à...  \n",
       "1  Miguel √Ångel F√©lix Gallardo (born January 8, 1...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load example dataset (FactScore)\n",
    "factscore = load_example_dataset(\"factscore\", n=2)[[\"hundredw_prompt\", \"wikipedia_text\"]].rename(columns={\"hundredw_prompt\": \"prompt\"})\n",
    "factscore.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use `AzureChatOpenAI` to instantiate our LLM, but any [LangChain Chat Model](https://js.langchain.com/docs/integrations/chat/) may be used. Be sure to **replace with your LLM of choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install langchain-openai\n",
    "\n",
    "## User to populate .env file with API credentials\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-4o\",\n",
    "    openai_api_type=\"azure\",\n",
    "    openai_api_version=\"2024-12-01-preview\",\n",
    "    temperature=1,  # User to set temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Generate LLM Responses and Claim/Sentence-Level Confidence Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `LongTextUQ()` - Generate long-text LLM responses, decompose into claims or sentences, and measure entailment among sampled responses.\n",
    "\n",
    "![Sample Image](https://raw.githubusercontent.com/cvs-health/uqlm/longform-uq-resolved/assets/images/luq_example.png)\n",
    "\n",
    "#### üìã Class Attributes\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 20%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Parameter</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Type & Default</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 55%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">llm</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BaseChatModel<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">A langchain llm `BaseChatModel`. User is responsible for specifying temperature and other relevant parameters to the constructor of the provided `llm` object.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">granularity</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str<br><code>default=\"claim\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to decompose and score at claim or sentence level granularity. Must be either \"claim\" or \"sentence\".</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">mode</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str<br><code>default=\"unit_response\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to implement unit-response (LUQ-style) scoring or matched-unit (LUQ-pair-style) scoring. Must be either \"unit_response\" (recommended) or \"matched_unit\".</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">scorers</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">List[str]<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies which black box (consistency) scorers to include. subset of {\"entailment\", \"noncontradiction\", \"contrasted_entailment\", \"bert_score\", \"cosine_sim\"}. If None, defaults to [\"entailment\"].</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">aggregation</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str<br><code>default=\"mean\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies how to aggregate claim/sentence-level scores to response-level scores. Must be one of 'min' or 'mean'.</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">response_refinement</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">bool<br><code>default=False</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to refine responses with uncertainty-aware decoding. This approach removes claims with confidence scores below the response_refinement_threshold and uses the claim_decomposition_llm to reconstruct the response from the retained claims. For more details, refer to Jiang et al., 2024: https://arxiv.org/abs/2410.20783</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">claim_filtering_scorer</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Optional[str]<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies which scorer to use to filter claims if response_refinement is True. If not provided, defaults to the first element of self.scorers.</td>\n",
    "  </tr>   \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">claim_decomposition_llm</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BaseChatModel<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">A langchain llm `BaseChatModel` to be used for decomposing responses into individual claims. Also used for claim refinement. If granularity=\"claim\" and claim_decomposition_llm is None, the provided `llm` will be used for claim decomposition.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">device</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str or torch.device<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies the device that NLI model use for prediction. If None, detects and returns the best available PyTorch device. Prioritizes CUDA (NVIDIA GPU), then MPS (macOS), then CPU.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">system_prompt</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str or None<br><code>default=\"You are a helpful assistant.\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Optional argument for user to provide custom system prompt for the LLM.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">max_calls_per_min</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">int<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies how many API calls to make per minute to avoid rate limit errors. By default, no limit is specified.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">use_n_param</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">bool<br><code>default=False</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to use <code>n</code> parameter for <code>BaseChatModel</code>. Not compatible with all <code>BaseChatModel</code> classes. If used, it speeds up the generation process substantially when <code>num_responses</code> is large.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">sampling_temperature</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">float<br><code>default=1</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">The 'temperature' parameter for LLM to use when generating sampled LLM responses. Must be greater than 0.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">nli_model_name</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str<br><code>default=\"microsoft/deberta-large-mnli\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies which NLI model to use. Must be acceptable input to <code>AutoTokenizer.from_pretrained()</code> and <code>AutoModelForSequenceClassification.from_pretrained()</code>.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">max_length</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">int<br><code>default=2000</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies the maximum allowed string length for LLM responses for NLI computation. Responses longer than this value will be truncated in NLI computations to avoid <code>OutOfMemoryError</code>.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### üîç Parameter Groups\n",
    "\n",
    "<div style=\"display: flex; gap: 20px; margin-bottom: 20px\">\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(0, 100, 200, 0.1); border-radius: 5px; border: 1px solid rgba(0, 100, 200, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üß† LLM-Specific</p>\n",
    "    <ul>\n",
    "      <li><code>llm</code></li>\n",
    "      <li><code>system_prompt</code></li>\n",
    "      <li><code>sampling_temperature</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(0, 200, 0, 0.1); border-radius: 5px; border: 1px solid rgba(0, 200, 0, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üìä Confidence Scores</p>\n",
    "    <ul>\n",
    "      <li><code>granularity</code></li>\n",
    "      <li><code>scorers</code></li>\n",
    "      <li><code>mode</code></li>\n",
    "      <li><code>aggregation</code></li>\n",
    "      <li><code>response_refinement</code></li>        \n",
    "      <li><code>response_refinement_threshold</code></li>  \n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(200, 150, 0, 0.1); border-radius: 5px; border: 1px solid rgba(200, 150, 0, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üñ•Ô∏è Hardware</p>\n",
    "    <ul>\n",
    "      <li><code>device</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(200, 0, 200, 0.1); border-radius: 5px; border: 1px solid rgba(200, 0, 200, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">‚ö° Performance</p>\n",
    "    <ul>\n",
    "      <li><code>max_calls_per_min</code></li>\n",
    "      <li><code>use_n_param</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim_filtering_scorer is not specified for response_refinement. Defaulting to entailment.\n"
     ]
    }
   ],
   "source": [
    "luq = LongTextUQ(\n",
    "    llm=llm,\n",
    "    granularity=\"claim\",  # 'claim' recommended\n",
    "    aggregation=\"mean\",  # switch to 'min' for more conservative scoring\n",
    "    response_refinement=True,  # whether to filter out low-confidence claims\n",
    "    max_calls_per_min=80,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Class Methods\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Method</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 75%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description & Parameters</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BlackBoxUQ.generate_and_score</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Generate LLM responses, sampled LLM (candidate) responses, and compute confidence scores for the provided prompts.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>prompts</code> - (<strong>List[str] or List[List[BaseMessage]]</strong>) A list of input prompts for the model.</li>\n",
    "        <li><code>num_responses</code> - (<strong>int, default=5</strong>) The number of sampled responses used to compute consistency.</li>\n",
    "        <li><code>response_refinement_threshold</code> - (<strong>float, default=1/3</strong>) Threshold for uncertainty-aware filtering. Claims with confidence scores below this threshold are dropped from the refined response. Only used if response_refinement is True.</li>        \n",
    "        <li><code>show_progress_bars</code> - (<strong>bool, default=True</strong>) If True, displays a progress bar while generating and scoring responses.</li>        \n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (prompts, responses, sampled responses, and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Complete end-to-end uncertainty quantification when starting with prompts.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BlackBoxUQ.score</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Compute confidence scores on provided LLM responses. Should only be used if responses and sampled responses are already generated.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>responses</code> - (<strong>List[str]</strong>) A list of LLM responses for the prompts.</li>\n",
    "        <li><code>sampled_responses</code> - (<strong>List[List[str]]</strong>) A list of lists of sampled LLM responses for each prompt. Used to compute consistency scores by comparing to the corresponding response from <code>responses</code>.</li>\n",
    "        <li><code>response_refinement_threshold</code> - (<strong>float, default=1/3</strong>) Threshold for uncertainty-aware filtering. Claims with confidence scores below this threshold are dropped from the refined response. Only used if response_refinement is True.</li>        \n",
    "        <li><code>show_progress_bars</code> - (<strong>bool, default=True</strong>) If True, displays a progress bar while scoring responses.</li>  \n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (responses, sampled responses, and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Computing uncertainty scores when responses are already generated elsewhere.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945fa91f0eaa4c14b8f98cbe5773d6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = await luq.generate_and_score(\n",
    "    prompts=factscore.prompt.to_list(),\n",
    "    num_responses=5,  # choose num_responses based on cost and latency requirements (higher means better hallucination detection but more cost and latency)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>sampled_responses</th>\n",
       "      <th>entailment</th>\n",
       "      <th>claims_data</th>\n",
       "      <th>refined_response</th>\n",
       "      <th>refined_entailment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me a bio of Suthida within 100 words.\\n</td>\n",
       "      <td>Suthida Bajrasudhabimalalakshana, born Suthida...</td>\n",
       "      <td>[Queen Suthida Bajrasudhabimalalakshana, born ...</td>\n",
       "      <td>0.495344</td>\n",
       "      <td>[{'claim': 'Suthida Bajrasudhabimalalakshana w...</td>\n",
       "      <td>Suthida Bajrasudhabimalalakshana, born on June...</td>\n",
       "      <td>0.713021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tell me a bio of Miguel √Ångel F√©lix Gallardo w...</td>\n",
       "      <td>Miguel √Ångel F√©lix Gallardo, born on January 8...</td>\n",
       "      <td>[Miguel √Ångel F√©lix Gallardo, born January 8, ...</td>\n",
       "      <td>0.553070</td>\n",
       "      <td>[{'claim': 'Miguel √Ångel F√©lix Gallardo was bo...</td>\n",
       "      <td>Miguel √Ångel F√©lix Gallardo, born on January 8...</td>\n",
       "      <td>0.702980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0       Tell me a bio of Suthida within 100 words.\\n   \n",
       "1  Tell me a bio of Miguel √Ångel F√©lix Gallardo w...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Suthida Bajrasudhabimalalakshana, born Suthida...   \n",
       "1  Miguel √Ångel F√©lix Gallardo, born on January 8...   \n",
       "\n",
       "                                   sampled_responses  entailment  \\\n",
       "0  [Queen Suthida Bajrasudhabimalalakshana, born ...    0.495344   \n",
       "1  [Miguel √Ångel F√©lix Gallardo, born January 8, ...    0.553070   \n",
       "\n",
       "                                         claims_data  \\\n",
       "0  [{'claim': 'Suthida Bajrasudhabimalalakshana w...   \n",
       "1  [{'claim': 'Miguel √Ångel F√©lix Gallardo was bo...   \n",
       "\n",
       "                                    refined_response  refined_entailment  \n",
       "0  Suthida Bajrasudhabimalalakshana, born on June...            0.713021  \n",
       "1  Miguel √Ångel F√©lix Gallardo, born on January 8...            0.702980  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = results.to_df()\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how the response refinement operates, let's view an example. We first view the fine-grained claim-level data, including the claims in the original response, the claim-level confidence scores, and whether each claim was removed during the response refinement process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim': 'Suthida Bajrasudhabimalalakshana was born Suthida Tidjai.',\n",
       "  'removed': True,\n",
       "  'entailment': 0.05828192420303822},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana was born on June 3, 1978.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.8498632729053497},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana was born in Hat Yai, Thailand.',\n",
       "  'removed': True,\n",
       "  'entailment': 0.004689936758950353},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana is the Queen consort of King Maha Vajiralongkorn.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.8190288722515107},\n",
       " {'claim': 'King Maha Vajiralongkorn is the King of Thailand.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.7976670324802398},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana is the Queen consort of Thailand.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.8245501160621643},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana has a background in aviation.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.401039526052773},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana worked as a flight attendant.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.48277700550388547},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana worked for Thai Airways.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.39189698969712483},\n",
       " {'claim': \"Suthida Bajrasudhabimalalakshana served in the King's guard.\",\n",
       "  'removed': False,\n",
       "  'entailment': 0.8247375726699829},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana rose through the ranks.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.7674157708883286},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana became a general.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.768857079744339},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana was appointed commander.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.5616361608728766},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana was appointed commander of the Royal Thai Aide-de-Camp Department.',\n",
       "  'removed': True,\n",
       "  'entailment': 0.024336618301458657},\n",
       " {'claim': 'On May 1, 2019, Suthida Bajrasudhabimalalakshana married King Vajiralongkorn.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.8207650303840637},\n",
       " {'claim': 'On May 1, 2019, Suthida Bajrasudhabimalalakshana became Queen.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.8551954448223114},\n",
       " {'claim': 'Suthida Bajrasudhabimalalakshana became Queen officially.',\n",
       "  'removed': False,\n",
       "  'entailment': 0.816862803697586},\n",
       " {'claim': \"Suthida Bajrasudhabimalalakshana's role includes various ceremonial functions.\",\n",
       "  'removed': True,\n",
       "  'entailment': 0.11837605498731137},\n",
       " {'claim': \"Suthida Bajrasudhabimalalakshana's role includes supporting royal initiatives.\",\n",
       "  'removed': True,\n",
       "  'entailment': 0.20340995360165834},\n",
       " {'claim': \"Suthida Bajrasudhabimalalakshana's role focuses on social welfare.\",\n",
       "  'removed': True,\n",
       "  'entailment': 0.0026208236697129903},\n",
       " {'claim': \"Suthida Bajrasudhabimalalakshana's role focuses on community welfare.\",\n",
       "  'removed': True,\n",
       "  'entailment': 0.008206463628448546}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View fine-grained claim data for the first response\n",
    "result_df.claims_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Illustrate claim removal for refined responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine a particular claim in the response that was removed because its confidence score was too low. Let's see how this is reflected in the original vs. the refined response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Original response:**\n",
      " Suthida Bajrasudhabimalalakshana, born Suthida Tidjai on June 3, 1978, in Hat Yai, Thailand, is the Queen consort of King Maha Vajiralongkorn of Thailand. She has a background in aviation, having worked as a flight attendant for Thai Airways and later serving in the King's guard. Suthida rose through the ranks, becoming a general, and was appointed commander of the Royal Thai Aide-de-Camp Department. On May 1, 2019, ahead of the coronation, she married King Vajiralongkorn, officially becoming Queen. Her role includes various ceremonial functions and supporting royal initiatives, focusing on social and community welfare.\n",
      " \n",
      "**Example claim to be removed:**\n",
      " Suthida Bajrasudhabimalalakshana's role focuses on community welfare.\n",
      " \n",
      "**Refined response:**\n",
      " Suthida Bajrasudhabimalalakshana, born on June 3, 1978, is the Queen consort of King Maha Vajiralongkorn, who presides as the King of Thailand. Before her marriage to King Vajiralongkorn on May 1, 2019, which marked her official ascension to Queen, Suthida had a distinguished career in aviation, serving as a flight attendant with Thai Airways. Her career took a significant turn when she joined the King's guard, where she demonstrated exceptional leadership skills and ascended through the ranks to eventually become a general and commander. This remarkable journey, from aviation to military service, underscores her multifaceted capabilities and dedication to service.\n"
     ]
    }
   ],
   "source": [
    "print(f\"**Original response:**\\n {result_df.response[0]}\")\n",
    "print(\" \")\n",
    "print(f\"**Example claim to be removed:**\\n {result_df.claims_data[0][-1]['claim']}\")  # update claim index to ensure you inspect a removed claim\n",
    "print(\" \")\n",
    "print(f\"**Refined response:**\\n {result_df.refined_response[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Evaluate Hallucination Detection Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate hallucination detection performance, we 'grade' the atomic claims in the responses against an answer key. Here, we use UQLM's out-of-the-box `FactScoreGrader`, which can be used with [LangChain Chat Model](https://js.langchain.com/docs/integrations/chat/). **If you are using your own prompts/questions, be sure to update the grading method accordingly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up the LLM grader\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "gemini_flash = ChatVertexAI(model=\"gemini-2.5-flash\")\n",
    "grader = FactScoreGrader(llm=gemini_flash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before grading, we need to have claims formatted in list of lists where each interior list corresponds to a generated response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert claims to list of lists\n",
    "claims_data_lists = claims_dicts_to_lists(result_df.claims_data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grade orinal responses against the answer key using the grader\n",
    "result_df[\"claim_grades\"] = await grader.grade_claims(claim_sets=claims_data_lists[\"claim\"], answers=factscore[\"wikipedia_text\"].to_list())\n",
    "result_df[\"answer\"] = factscore[\"wikipedia_text\"]\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before grading, we need to have claims formatted in list of lists where each interior list corresponds to a generated response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_claim_scores, all_claim_grades = [], []\n",
    "for i in range(len(result_df)):\n",
    "    all_claim_scores.extend(claims_data_lists[\"entailment\"][i])\n",
    "    all_claim_grades.extend(result_df[\"claim_grades\"][i])\n",
    "\n",
    "print(f\"\"\"Baseline LLM accuracy: {np.mean(all_claim_grades)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Claim-Level Hallucination Detection AUROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate fine-grained hallucination detection performance, we compute AUROC of claim-level hallucination detection. Below, we plot the ROC curve and report these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true=all_claim_grades, y_score=all_claim_scores)\n",
    "roc_auc = roc_auc_score(y_true=all_claim_grades, y_score=all_claim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Gains from Uncertainty-Aware Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we evaluate the gains from uncertainty-aware decoding (UAD) by measuring the factual precision over claims at various filtering thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model_accuracies(scores=all_claim_scores, correct_indicators=all_claim_grades, title=\"LLM Accuracy by Claim Confidence Threshold\", display_percentage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we have selected a threshold of 0.35, we can measure LLM accuracy with and without UAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresh = 1 / 3\n",
    "filtered_grades, filtered_scores = [], []\n",
    "for grade, score in zip(all_claim_grades, all_claim_scores):\n",
    "    if score > thresh:\n",
    "        filtered_grades.append(grade)\n",
    "        filtered_scores.append(score)\n",
    "\n",
    "print(f\"Baseline LLM factual precision: {np.mean(all_claim_grades)}\")\n",
    "print(f\"UAD-Improved LLM factual precision: {np.mean(filtered_grades)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Scorer Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long-form uncertainty quantification implements a three-stage pipeline after response generation:\n",
    "\n",
    "1. Response Decomposition: The response $y$ is decomposed into units (claims or sentences), where a unit as denoted as $s$.\n",
    "\n",
    "2. Unit-Level Confidence Scoring: Confidence scores are computed using function $c_g(s;\\cdot) \\in [0, 1]$. Higher scores indicate greater likelihood of factual correctness. Units with scores below threshold $\\tau$ are flagged as potential hallucinations.\n",
    "\n",
    "3. Response-Level Aggregation: Unit scores are combined to provide an overall response confidence.\n",
    "\n",
    "The Long-text UQ (LUQ) approach demonstrated here is adapted from [Zhang et al., 2024](https://arxiv.org/abs/2403.20279). Similar to standard black-box UQ, this approach requires generating a original response and sampled candidate responses to the same prompt. The original response is then decomposed into units (claims or sentences). Unit-level confidence scores are then obtained by averaging entailment probabilities across candidate responses:\n",
    "\n",
    "$$c_g(s; \\mathbf{y}_{\\text{cand}}) = \\frac{1}{m} \\sum_{j=1}^m P(\\text{entail}|y_j, s)$$\n",
    "\n",
    "where $\\mathbf{y}^{(s)}_{\\text{cand}} = {y_1^{(s)}, ..., y_m^{(s)}}$ are $m$ candidate responses, and $P(\\text{entail}|y_j, s)$ denotes the NLI-estimated probability that $s$ is entailed in $y_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬© 2025 CVS Health and/or one of its affiliates. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "uqlm_my_test",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "uqlm_my_test",
   "language": "python",
   "name": "uqlm_my_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
