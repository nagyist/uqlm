{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ LLM-as-a-Judge\n",
    "\n",
    "<div style=\"background-color: rgba(200, 200, 200, 0.1); padding: 20px; border-radius: 8px; margin-bottom: 20px; border: 1px solid rgba(127, 127, 127, 0.2); max-width: 100%; overflow-wrap: break-word;\">\n",
    "  <p style=\"font-size: 16px; line-height: 1.6\">\n",
    "   LLM-as-a-Judge scorers use one or more LLMs to evaluate the reliability of the original LLM's response. They offer high customizability through prompt engineering and the choice of judge LLM(s). Below is a list of the available scorers:\n",
    "  </p>\n",
    "\n",
    "*   Categorical LLM-as-a-Judge ([Manakul et al., 2023](https://arxiv.org/abs/2303.08896); [Chen & Mueller, 2023](https://arxiv.org/abs/2308.16175); [Luo et al., 2023](https://arxiv.org/pdf/2303.15621))\n",
    "*   Continuous LLM-as-a-Judge ([Xiong et al., 2024](https://arxiv.org/pdf/2306.13063))\n",
    "*   Panel of LLM Judges ([Verga et al., 2024](https://arxiv.org/abs/2404.18796))\n",
    "    \n",
    "</div>\n",
    "\n",
    "## üìä What You'll Do in This Demo\n",
    "\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>1</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section1>Set up LLM and prompts.</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Set up LLM instance and load example data prompts.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>2</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section2>Generate LLM Responses and Confidence Scores</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Generate and score LLM responses to the example questions using the <code>LLMPanel()</code> class.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 25px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>3</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section3>Evaluate Hallucination Detection Performance</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Compute precision, recall, and F1-score of hallucination detection.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "## ‚öñÔ∏è Advantages & Limitations\n",
    "\n",
    "<div style=\"display: flex; gap: 20px\">\n",
    "  <div style=\"flex: 1; background-color: rgba(0, 200, 0, 0.1); padding: 15px; border-radius: 8px; border: 1px solid rgba(0, 200, 0, 0.2)\">\n",
    "    <h3 style=\"color: #2e8b57; margin-top: 0\">Pros</h3>\n",
    "    <ul style=\"margin-bottom: 0\">\n",
    "      <li><strong>Universal Compatibility:</strong> Works with any LLM.</li>\n",
    "      <li><strong>Highly Customizable:</strong> Use any LLM as a judge and tailor instruction prompts for specific use cases.</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"flex: 1; background-color: rgba(200, 0, 0, 0.1); padding: 15px; border-radius: 8px; border: 1px solid rgba(200, 0, 0, 0.2)\">\n",
    "    <h3 style=\"color: #b22222; margin-top: 0\">Cons</h3>\n",
    "    <ul style=\"margin-bottom: 0\">\n",
    "      <li><strong>Added cost:</strong> Requires additional LLM calls for the judge LLM(s).</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from uqlm import LLMPanel\n",
    "from uqlm.utils import load_example_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Set up LLM and Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will illustrate this approach using a set of math questions from the [SVAMP benchmark](https://arxiv.org/abs/2103.07191). To implement with your use case, simply **replace the example prompts with your data**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset - nq_open...\n",
      "Processing dataset...\n",
      "Dataset ready!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when was the last time anyone was on the moon</td>\n",
       "      <td>[14 december 1972 utc, december 1972]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who wrote he ain't heavy he's my brother lyrics</td>\n",
       "      <td>[bobby scott, bob russell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how many seasons of the bastard executioner ar...</td>\n",
       "      <td>[one, one season]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when did the eagles win last super bowl</td>\n",
       "      <td>[2017]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who won last year's ncaa women's basketball</td>\n",
       "      <td>[south carolina]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0      when was the last time anyone was on the moon   \n",
       "1    who wrote he ain't heavy he's my brother lyrics   \n",
       "2  how many seasons of the bastard executioner ar...   \n",
       "3            when did the eagles win last super bowl   \n",
       "4        who won last year's ncaa women's basketball   \n",
       "\n",
       "                                  answer  \n",
       "0  [14 december 1972 utc, december 1972]  \n",
       "1             [bobby scott, bob russell]  \n",
       "2                      [one, one season]  \n",
       "3                                 [2017]  \n",
       "4                       [south carolina]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load example dataset (NQ_OPEN)\n",
    "nq_open = load_example_dataset(\"nq_open\", n=75)\n",
    "nq_open.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "INSTRUCTION = \"You will be given a question. Return only the answer as concisely as possible without providing an explanation.\\n\"\n",
    "prompts = [INSTRUCTION + prompt for prompt in nq_open.question]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use `ChatOllama` to instantiate our LLMs, but any [LangChain Chat Model](https://js.langchain.com/docs/integrations/chat/) may be used. Be sure to **replace with your LLM of choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install langchain-ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "ollama_llama = ChatOllama(model=\"llama2\")\n",
    "ollama_mistral = ChatOllama(model=\"mistral\")\n",
    "ollama_qwen = ChatOllama(model=\"qwen3\")\n",
    "# ollama_deepseek = ChatOllama(model=\"deepseek-r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative setup with API models\n",
    "\n",
    "## ChatVertexAI example\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install langchain-google-vertexai\n",
    "# from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "# gemini_pro = ChatVertexAI(model_name=\"gemini-2.5-pro\")\n",
    "# gemini_flash = ChatVertexAI(model_name=\"gemini-2.5-flash\")\n",
    "\n",
    "\n",
    "## AzureChatOpenAI example\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install langchain-openai\n",
    "\n",
    "# # User to populate .env file with API credentials\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# load_dotenv(find_dotenv())\n",
    "# original_llm = AzureChatOpenAI(\n",
    "#     deployment_name=\"gpt-4o\",\n",
    "#     openai_api_type=\"azure\",\n",
    "#     openai_api_version=\"2024-02-15-preview\",\n",
    "#     temperature=1,  # User to set temperature\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Generate responses and confidence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `LLMPanel()` - Class for aggregating multiple instances of LLMJudge using average, min, max, or majority voting\n",
    "\n",
    "![Sample Image](https://raw.githubusercontent.com/cvs-health/uqlm/develop/assets/images/judges_graphic.png)\n",
    "\n",
    "#### üìã Class Attributes\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 20%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Parameter</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Type & Default</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 55%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">judges</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">list of LLMJudge or BaseChatModel<br><code></code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Judges to use. If BaseChatModel, LLMJudge is instantiated using default parameters.</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">llm</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BaseChatModel<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">A langchain llm `BaseChatModel`. User is responsible for specifying temperature and other relevant parameters to the constructor of the provided `llm` object.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">system_prompt</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str or None<br><code>default=\"You are a helpful assistant.\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Optional argument for user to provide custom system prompt for the LLM.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">max_calls_per_min</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">int<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies how many API calls to make per minute to avoid rate limit errors. By default, no limit is specified.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">explanations</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">bool<br><code>default=False</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Whether to include explanations from judges alongside scores. When True, judges provide reasoning for their scores.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">additional_context</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str or None<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Optional argument to provide additional context to inform LLM-as-a-Judge evaluations.</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "#### üîç Parameter Groups\n",
    "\n",
    "<div style=\"display: flex; gap: 20px; margin-bottom: 20px\">\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(0, 100, 200, 0.1); border-radius: 5px; border: 1px solid rgba(0, 100, 200, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üß† LLM-Specific</p>\n",
    "    <ul>\n",
    "      <li><code>llm</code></li>\n",
    "      <li><code>system_prompt</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(0, 200, 0, 0.1); border-radius: 5px; border: 1px solid rgba(0, 200, 0, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üìä Confidence Scores</p>\n",
    "    <ul>\n",
    "      <li><code>judges</code></li>\n",
    "      <li><code>explanations</code></li>        \n",
    "      <li><code>additional_context</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(200, 0, 200, 0.1); border-radius: 5px; border: 1px solid rgba(200, 0, 200, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">‚ö° Performance</p>\n",
    "    <ul>\n",
    "      <li><code>max_calls_per_min</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "#### üíª Usage Examples\n",
    "\n",
    "```python\n",
    "# Basic usage with single self-judge parameters\n",
    "panel = LLMPanel(llm=llm, judges=[llm])\n",
    "\n",
    "# Using two judges with default parameters\n",
    "panel = LLMPanel(llm=llm, judges=[llm, llm2])\n",
    "\n",
    "# Using two judges with default parameters\n",
    "panel = LLMPanel(llm=llm, judges=[llm, llm2])\n",
    "\n",
    "# Using judges with explanations enabled\n",
    "panel_with_explanations = LLMPanel(\n",
    "    llm=llm, judges=[llm, llm2], explanations=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "judges = [ollama_mistral, ollama_llama, ollama_qwen]\n",
    "additional_context = \"You are an expert in pop culture and current events. Your task is to evaluate the correctness of proposed answers to provided questions.\"\n",
    "\n",
    "# Option 1: With explanations\n",
    "panel = LLMPanel(llm=ollama_mistral, judges=judges, additional_context=additional_context, explanations=True)\n",
    "\n",
    "# Option 2: Without explanations\n",
    "# panel = LLMPanel(llm=ollama_mistral, judges=judges, additional_context=additional_context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Class Methods\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Method</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 75%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description & Parameters</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">LLMPanel.generate_and_score</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Generate responses to provided prompts and use panel to of judges to score responses for correctness.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>prompts</code> - (<strong>list of str</strong>) A list of input prompts for the model.</li>\n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (prompts, responses, sampled responses, and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Complete end-to-end uncertainty quantification when starting with prompts.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">LLMPanel.score</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Use panel to of judges to score provided responses for correctness. Use if responses are already generated. Otherwise, use `generate_and_score`.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>prompts</code> - (<strong>list of str</strong>) A list of input prompts for the model.</li>\n",
    "        <li><code>responses</code> - (<strong>list of str</strong>) A list of LLM responses for the prompts.</li>\n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (responses and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Computing uncertainty scores when responses are already generated elsewhere.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9e4462f2584ad98e45feffe63f9b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d164e2882d794079aa50075356d0bc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = await panel.generate_and_score(prompts=prompts)\n",
    "\n",
    "# option 2: provide pre-generated responses with score method\n",
    "# result = await panel.score(prompts=prompts, responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>judge_1</th>\n",
       "      <th>judge_1_explanation</th>\n",
       "      <th>judge_2</th>\n",
       "      <th>judge_2_explanation</th>\n",
       "      <th>judge_3</th>\n",
       "      <th>judge_3_explanation</th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You will be given a question. Return only the ...</td>\n",
       "      <td>The last manned mission to the moon was Apoll...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The proposed answer correctly identifies Apoll...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The last manned mission to the moon was Apollo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Apollo 17 was indeed the last manned mission t...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You will be given a question. Return only the ...</td>\n",
       "      <td>The lyrics \"He Ain't Heavy, He's My Brother\" ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The proposed answer correctly identifies Bob R...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The lyrics \"He Ain't Heavy, He's My Brother\" w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The answer correctly identifies Bob Russell an...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You will be given a question. Return only the ...</td>\n",
       "      <td>There are 10 episodes in one season for The B...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The proposed answer correctly states that ther...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The information provided in the question is co...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The answer correctly states that \"The Bastard ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You will be given a question. Return only the ...</td>\n",
       "      <td>The Eagles won their last Super Bowl in 2017 ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The Eagles indeed won Super Bowl LII in 2017.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No explanation provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The Eagles won Super Bowl LII in 2018, not 2017.</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You will be given a question. Return only the ...</td>\n",
       "      <td>Stanford University won last year's (2021) NC...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>According to the provided information, Stanfor...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The proposed answer \"Stanford University\" is c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The 2021 NCAA Women's Basketball Championship ...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  You will be given a question. Return only the ...   \n",
       "1  You will be given a question. Return only the ...   \n",
       "2  You will be given a question. Return only the ...   \n",
       "3  You will be given a question. Return only the ...   \n",
       "4  You will be given a question. Return only the ...   \n",
       "\n",
       "                                            response  judge_1  \\\n",
       "0   The last manned mission to the moon was Apoll...      1.0   \n",
       "1   The lyrics \"He Ain't Heavy, He's My Brother\" ...      1.0   \n",
       "2   There are 10 episodes in one season for The B...      1.0   \n",
       "3   The Eagles won their last Super Bowl in 2017 ...      1.0   \n",
       "4   Stanford University won last year's (2021) NC...      1.0   \n",
       "\n",
       "                                 judge_1_explanation  judge_2  \\\n",
       "0  The proposed answer correctly identifies Apoll...      1.0   \n",
       "1  The proposed answer correctly identifies Bob R...      1.0   \n",
       "2  The proposed answer correctly states that ther...      1.0   \n",
       "3      The Eagles indeed won Super Bowl LII in 2017.      1.0   \n",
       "4  According to the provided information, Stanfor...      1.0   \n",
       "\n",
       "                                 judge_2_explanation  judge_3  \\\n",
       "0  The last manned mission to the moon was Apollo...      1.0   \n",
       "1  The lyrics \"He Ain't Heavy, He's My Brother\" w...      1.0   \n",
       "2  The information provided in the question is co...      1.0   \n",
       "3                            No explanation provided      0.0   \n",
       "4  The proposed answer \"Stanford University\" is c...      0.0   \n",
       "\n",
       "                                 judge_3_explanation       avg  max  min  \\\n",
       "0  Apollo 17 was indeed the last manned mission t...  1.000000  1.0  1.0   \n",
       "1  The answer correctly identifies Bob Russell an...  1.000000  1.0  1.0   \n",
       "2  The answer correctly states that \"The Bastard ...  1.000000  1.0  1.0   \n",
       "3   The Eagles won Super Bowl LII in 2018, not 2017.  0.666667  1.0  0.0   \n",
       "4  The 2021 NCAA Women's Basketball Championship ...  0.666667  1.0  0.0   \n",
       "\n",
       "   median  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.to_df()\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Evaluate Hallucination Detection Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate hallucination detection performance, we 'grade' the responses against an answer key. Note the `math_postprocessor` is specific to our use case (math questions). **If you are using your own prompts/questions, update the grading method accordingly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>judge_1</th>\n",
       "      <th>judge_1_explanation</th>\n",
       "      <th>judge_2</th>\n",
       "      <th>judge_2_explanation</th>\n",
       "      <th>judge_3</th>\n",
       "      <th>judge_3_explanation</th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>median</th>\n",
       "      <th>answer</th>\n",
       "      <th>response_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You will be given a question. Return only the ...</td>\n",
       "      <td>The last manned mission to the moon was Apoll...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The proposed answer correctly identifies Apoll...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The last manned mission to the moon was Apollo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Apollo 17 was indeed the last manned mission t...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[14 december 1972 utc, december 1972]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You will be given a question. Return only the ...</td>\n",
       "      <td>The lyrics \"He Ain't Heavy, He's My Brother\" ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The proposed answer correctly identifies Bob R...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The lyrics \"He Ain't Heavy, He's My Brother\" w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The answer correctly identifies Bob Russell an...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[bobby scott, bob russell]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You will be given a question. Return only the ...</td>\n",
       "      <td>There are 10 episodes in one season for The B...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The proposed answer correctly states that ther...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The information provided in the question is co...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The answer correctly states that \"The Bastard ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[one, one season]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You will be given a question. Return only the ...</td>\n",
       "      <td>The Eagles won their last Super Bowl in 2017 ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The Eagles indeed won Super Bowl LII in 2017.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No explanation provided</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The Eagles won Super Bowl LII in 2018, not 2017.</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[2017]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You will be given a question. Return only the ...</td>\n",
       "      <td>Stanford University won last year's (2021) NC...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>According to the provided information, Stanfor...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The proposed answer \"Stanford University\" is c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The 2021 NCAA Women's Basketball Championship ...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[south carolina]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  You will be given a question. Return only the ...   \n",
       "1  You will be given a question. Return only the ...   \n",
       "2  You will be given a question. Return only the ...   \n",
       "3  You will be given a question. Return only the ...   \n",
       "4  You will be given a question. Return only the ...   \n",
       "\n",
       "                                            response  judge_1  \\\n",
       "0   The last manned mission to the moon was Apoll...      1.0   \n",
       "1   The lyrics \"He Ain't Heavy, He's My Brother\" ...      1.0   \n",
       "2   There are 10 episodes in one season for The B...      1.0   \n",
       "3   The Eagles won their last Super Bowl in 2017 ...      1.0   \n",
       "4   Stanford University won last year's (2021) NC...      1.0   \n",
       "\n",
       "                                 judge_1_explanation  judge_2  \\\n",
       "0  The proposed answer correctly identifies Apoll...      1.0   \n",
       "1  The proposed answer correctly identifies Bob R...      1.0   \n",
       "2  The proposed answer correctly states that ther...      1.0   \n",
       "3      The Eagles indeed won Super Bowl LII in 2017.      1.0   \n",
       "4  According to the provided information, Stanfor...      1.0   \n",
       "\n",
       "                                 judge_2_explanation  judge_3  \\\n",
       "0  The last manned mission to the moon was Apollo...      1.0   \n",
       "1  The lyrics \"He Ain't Heavy, He's My Brother\" w...      1.0   \n",
       "2  The information provided in the question is co...      1.0   \n",
       "3                            No explanation provided      0.0   \n",
       "4  The proposed answer \"Stanford University\" is c...      0.0   \n",
       "\n",
       "                                 judge_3_explanation       avg  max  min  \\\n",
       "0  Apollo 17 was indeed the last manned mission t...  1.000000  1.0  1.0   \n",
       "1  The answer correctly identifies Bob Russell an...  1.000000  1.0  1.0   \n",
       "2  The answer correctly states that \"The Bastard ...  1.000000  1.0  1.0   \n",
       "3   The Eagles won Super Bowl LII in 2018, not 2017.  0.666667  1.0  0.0   \n",
       "4  The 2021 NCAA Women's Basketball Championship ...  0.666667  1.0  0.0   \n",
       "\n",
       "   median                                 answer  response_correct  \n",
       "0     1.0  [14 december 1972 utc, december 1972]              True  \n",
       "1     1.0             [bobby scott, bob russell]              True  \n",
       "2     1.0                      [one, one season]              True  \n",
       "3     1.0                                 [2017]              True  \n",
       "4     1.0                       [south carolina]             False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Populate correct answers and grade responses\n",
    "result_df[\"answer\"] = nq_open.answer\n",
    "\n",
    "\n",
    "def short_answer_grader(response: str, possible_answers) -> bool:\n",
    "    \"\"\"Check entailment of possible answers in a response\"\"\"\n",
    "    # convert string to list if needed\n",
    "    if isinstance(possible_answers, str):\n",
    "        possible_answers = [possible_answers]\n",
    "    #  iterate over the list items\n",
    "    for s in possible_answers:\n",
    "        if s.lower() in response.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Grade responses against correct answers\n",
    "result_df[\"response_correct\"] = [short_answer_grader(response=r, possible_answers=a) for r, a in zip(result_df[\"response\"], nq_open[\"answer\"])]\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge 1 precision: 0.48484848484848486\n",
      "Judge 1 recall: 0.9411764705882353\n",
      "Judge 1 f1-score: 0.64\n",
      " \n",
      "Judge 2 precision: 0.4492753623188406\n",
      "Judge 2 recall: 0.9117647058823529\n",
      "Judge 2 f1-score: 0.6019417475728155\n",
      " \n",
      "Judge 3 precision: 0.5263157894736842\n",
      "Judge 3 recall: 0.8823529411764706\n",
      "Judge 3 f1-score: 0.6593406593406593\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# evaluate precision, recall, and f1-score of Semantic Entropy's predictions of correctness\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for ind in [1, 2, 3]:\n",
    "    y_pred = [(s > 0) * 1 for s in result_df[f\"judge_{str(ind)}\"]]\n",
    "    y_true = result_df.response_correct\n",
    "    print(f\"Judge {ind} precision: {precision_score(y_true=y_true, y_pred=y_pred)}\")\n",
    "    print(f\"Judge {ind} recall: {recall_score(y_true=y_true, y_pred=y_pred)}\")\n",
    "    print(f\"Judge {ind} f1-score: {f1_score(y_true=y_true, y_pred=y_pred)}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. Scorer Definitions\n",
    "Under the LLM-as-a-Judge approach, either the same LLM that was used for generating the original responses or a different LLM is asked to form a judgment about a pre-generated response. Below, we define two LLM-as-a-Judge scorer templates. \n",
    "### Categorical Judge Template (`true_false_uncertain`)\n",
    "We follow the approach proposed by [Chen & Mueller, 2023](https://arxiv.org/abs/2308.16175) in which an LLM is instructed to score a question-answer concatenation as either  *incorrect*, *uncertain*, or *correct* using a carefully constructed prompt. These categories are respectively mapped to numerical scores of 0, 0.5, and 1. We denote the LLM-as-a-judge scorers as $J: \\mathcal{Y} \\xrightarrow[]{} \\{0, 0.5, 1\\}$. Formally, we can write this scorer function as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "J(y_i) = \\begin{cases}\n",
    "    0 & \\text{LLM states response is incorrect} \\\\\n",
    "    0.5 & \\text{LLM states that it is uncertain} \\\\\n",
    "    1 & \\text{LLM states response is correct}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "### Continuous Judge Template (`continuous`)\n",
    "For the continuous template, the LLM is asked to directly score a question-answer concatenation's correctness on a scale of 0 to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬© 2025 CVS Health and/or one of its affiliates. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "uqlm_my_test",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "uqlm_my_test",
   "language": "python",
   "name": "uqlm_my_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
