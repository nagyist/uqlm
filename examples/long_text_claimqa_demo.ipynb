{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Claim-QA Uncertainty Quantification (Long-Text)\n",
    "\n",
    "<div style=\"background-color: rgba(200, 200, 200, 0.1); padding: 20px; border-radius: 8px; margin-bottom: 20px; border: 1px solid rgba(127, 127, 127, 0.2); max-width: 97.5%; overflow-wrap: break-word;\">\n",
    "  <p style=\"font-size: 16px; line-height: 1.6\">\n",
    "     Claim-QA scorers, adapted as a generalization of long-form semantic entropy, are another method for detecting claim-level or sentence-level hallucinations in long-form LLM outputs. These scorers implement the following steps: decompose responses into granular units (sentences or claims), convert each claim or sentence to a question, sample LLM responses to those questions, and measure consistency among those answers to score the claim. The available scorers and papers from which they are adapted are below:\n",
    "  </p>\n",
    "      \n",
    "*   Long-form Semantic Entropy ([Farquhar et al., 2024](https://www.nature.com/articles/s41586-024-07421-0))\n",
    "*   Black-Box Generalizations of Long-form Semantic Entropy\n",
    "\n",
    "</div>\n",
    "\n",
    "## üìä What You'll Do in This Demo\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>1</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section1>Set up LLM and prompts.</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Set up LLM instance and load example data prompts.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 15px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>2</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section2>Generate LLM Responses and Confidence Scores</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Generate responses and compute claim-level confidence scores using the <code>LongTextQA()</code> class.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; margin-bottom: 25px; align-items: center\">\n",
    "  <div style=\"background-color: #34a853; color: white; border-radius: 50%; width: 30px; height: 30px; display: flex; justify-content: center; align-items: center; margin-right: 15px; flex-shrink: 0\"><strong>3</strong></div>\n",
    "  <div>\n",
    "    <p style=\"margin: 0; font-weight: bold\"><a href=#section3>Evaluate Hallucination Detection Performance</a></p>\n",
    "    <p style=\"margin: 0; color: rgba(95, 99, 104, 0.8)\">Grade claims with `FactScoreGrader` class and evaluate claim-level hallucination detection.</p>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "## ‚öñÔ∏è Advantages & Limitations\n",
    "\n",
    "<div style=\"display: flex; gap: 20px\">\n",
    "  <div style=\"flex: 1; background-color: rgba(0, 200, 0, 0.1); padding: 15px; border-radius: 8px; border: 1px solid rgba(0, 200, 0, 0.2)\">\n",
    "    <h3 style=\"color: #2e8b57; margin-top: 0\">Pros</h3>\n",
    "    <ul style=\"margin-bottom: 0\">\n",
    "      <li><strong>Universal Compatibility:</strong> Works with any LLM without requiring token probability access</li>\n",
    "      <li><strong>Fine-Grained Scoring:</strong> Score at sentence or claim-level to localize likely hallucinations</li>\n",
    "      <li><strong>Uncertainty-aware decoding:</strong> Improve factual precision by dropping high-uncertainty claims</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"flex: 1; background-color: rgba(200, 0, 0, 0.1); padding: 15px; border-radius: 8px; border: 1px solid rgba(200, 0, 0, 0.2)\">\n",
    "    <h3 style=\"color: #b22222; margin-top: 0\">Cons</h3>\n",
    "    <ul style=\"margin-bottom: 0\">\n",
    "      <li><strong>Higher Cost:</strong> Requires multiple generations per prompt</li>\n",
    "      <li><strong>Slower:</strong> Multiple generations and comparison calculations increase latency</li>\n",
    "      <li><strong>Complex:</strong> More complex than simpler methods offered by `LongTextUQ`.</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from uqlm import LongTextQA\n",
    "from uqlm.utils import claims_dicts_to_lists, load_example_dataset, plot_model_accuracies\n",
    "from uqlm.longform import FactScoreGrader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Set up LLM and Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will illustrate this approach using the [FactScore](https://github.com/shmsw25/FActScore/tree/main/factscore) longform QA dataset. To implement with your use case, simply **replace the example prompts with your data**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset - factscore...\n",
      "Processing dataset...\n",
      "Dataset ready!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>wikipedia_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tell me a bio of Suthida within 100 words.\\n</td>\n",
       "      <td>Suthida Bajrasudhabimalalakshana (Thai: ‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tell me a bio of Miguel √Ångel F√©lix Gallardo w...</td>\n",
       "      <td>Miguel √Ångel F√©lix Gallardo (born January 8, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell me a bio of Iggy Azalea within 100 words.\\n</td>\n",
       "      <td>Amethyst Amelia Kelly (born 7 June 1990), know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tell me a bio of Fernando da Costa Novaes with...</td>\n",
       "      <td>Fernando da Costa Novaes (April 6, 1927 ‚Äì Marc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tell me a bio of Jan Zamoyski within 100 words.\\n</td>\n",
       "      <td>Jan Sariusz Zamoyski (Latin: Ioannes Zamoyski ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0       Tell me a bio of Suthida within 100 words.\\n   \n",
       "1  Tell me a bio of Miguel √Ångel F√©lix Gallardo w...   \n",
       "2   Tell me a bio of Iggy Azalea within 100 words.\\n   \n",
       "3  Tell me a bio of Fernando da Costa Novaes with...   \n",
       "4  Tell me a bio of Jan Zamoyski within 100 words.\\n   \n",
       "\n",
       "                                      wikipedia_text  \n",
       "0  Suthida Bajrasudhabimalalakshana (Thai: ‡∏™‡∏°‡πÄ‡∏î‡πá‡∏à...  \n",
       "1  Miguel √Ångel F√©lix Gallardo (born January 8, 1...  \n",
       "2  Amethyst Amelia Kelly (born 7 June 1990), know...  \n",
       "3  Fernando da Costa Novaes (April 6, 1927 ‚Äì Marc...  \n",
       "4  Jan Sariusz Zamoyski (Latin: Ioannes Zamoyski ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load example dataset (FactScore)\n",
    "factscore = load_example_dataset(\"factscore\", n=30)[[\"hundredw_prompt\", \"wikipedia_text\"]].rename(columns={\"hundredw_prompt\": \"prompt\"})\n",
    "factscore.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use `ChatVertexAI` to instantiate our LLM, but any [LangChain Chat Model](https://js.langchain.com/docs/integrations/chat/) may be used. Be sure to **replace with your LLM of choice.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "gemini_flash = ChatVertexAI(model=\"gemini-2.5-flash\")\n",
    "gemini_flash_lite = ChatVertexAI(model=\"gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Generate LLM Responses and Claim/Sentence-Level Confidence Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `LongTextQA()` - Generate long-text LLM responses, decompose into claims or sentences, create questions for which those claims are the answers, and measure consistency in LLM responses to those questions.\n",
    "\n",
    "![Sample Image](https://raw.githubusercontent.com/cvs-health/uqlm/longform-uq-resolved/assets/images/claim_qa_graphic.png)\n",
    "\n",
    "#### üìã Class Attributes\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 20%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Parameter</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Type & Default</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 55%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">llm</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BaseChatModel<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">A langchain llm `BaseChatModel`. User is responsible for specifying temperature and other relevant parameters to the constructor of the provided `llm` object.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">granularity</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str<br><code>default=\"claim\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to decompose and score at claim or sentence level granularity. Must be either \"claim\" or \"sentence\".</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">scorers</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">List[str]<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies which black box (consistency) scorers to include. Must be subset of ['semantic_negentropy', 'noncontradiction', 'exact_match', 'bert_score', 'cosine_sim', 'entailment', 'semantic_sets_confidence']. If None, defaults to [\"entailment\"].</td>\n",
    "  </tr>    \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">aggregation</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str<br><code>default=\"mean\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies how to aggregate claim/sentence-level scores to response-level scores. Must be one of 'min' or 'mean'.</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">response_refinement</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">bool<br><code>default=False</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to refine responses with uncertainty-aware decoding. This approach removes claims with confidence scores below the response_refinement_threshold and uses the claim_decomposition_llm to reconstruct the response from the retained claims. For more details, refer to Jiang et al., 2024: https://arxiv.org/abs/2410.20783</td>\n",
    "  </tr>  \n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">claim_decomposition_llm</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BaseChatModel<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">A langchain llm `BaseChatModel` to be used for decomposing responses into individual claims. Also used for claim refinement. If granularity=\"claim\" and claim_decomposition_llm is None, the provided `llm` will be used for claim decomposition.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">question_generator_llm</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BaseChatModel<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">A langchain llm `BaseChatModel` to be used for decomposing responses into individual claims. Used for generating questions from claims or sentences in claim-QA approach. If None, defaults to claim_decomposition_llm.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">device</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str or torch.device<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies the device that NLI model use for prediction. If None, detects and returns the best available PyTorch device. Prioritizes CUDA (NVIDIA GPU), then MPS (macOS), then CPU.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">system_prompt</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str or None<br><code>default=\"You are a helpful assistant.\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Optional argument for user to provide custom system prompt for the LLM.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">max_calls_per_min</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">int<br><code>default=None</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies how many API calls to make per minute to avoid rate limit errors. By default, no limit is specified.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">use_n_param</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">bool<br><code>default=False</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies whether to use <code>n</code> parameter for <code>BaseChatModel</code>. Not compatible with all <code>BaseChatModel</code> classes. If used, it speeds up the generation process substantially when <code>num_responses</code> is large.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">sampling_temperature</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">float<br><code>default=1</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">The 'temperature' parameter for LLM to use when generating sampled LLM responses. Must be greater than 0.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">nli_model_name</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">str<br><code>default=\"microsoft/deberta-large-mnli\"</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies which NLI model to use. Must be acceptable input to <code>AutoTokenizer.from_pretrained()</code> and <code>AutoModelForSequenceClassification.from_pretrained()</code>.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">max_length</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">int<br><code>default=2000</code></td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">Specifies the maximum allowed string length for LLM responses for NLI computation. Responses longer than this value will be truncated in NLI computations to avoid <code>OutOfMemoryError</code>.</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "#### üîç Parameter Groups\n",
    "\n",
    "<div style=\"display: flex; gap: 20px; margin-bottom: 20px\">\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(0, 100, 200, 0.1); border-radius: 5px; border: 1px solid rgba(0, 100, 200, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üß† LLM-Specific</p>\n",
    "    <ul>\n",
    "      <li><code>llm</code></li>\n",
    "      <li><code>system_prompt</code></li>\n",
    "      <li><code>sampling_temperature</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(0, 200, 0, 0.1); border-radius: 5px; border: 1px solid rgba(0, 200, 0, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üìä Confidence Scores</p>\n",
    "    <ul>\n",
    "      <li><code>granularity</code></li>\n",
    "      <li><code>scorers</code></li>\n",
    "      <li><code>aggregation</code></li>\n",
    "      <li><code>num_questions</code></li>\n",
    "      <li><code>num_claim_qa_responses</code></li>\n",
    "      <li><code>response_refinement</code></li>        \n",
    "      <li><code>response_refinement_threshold</code></li>  \n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(200, 150, 0, 0.1); border-radius: 5px; border: 1px solid rgba(200, 150, 0, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">üñ•Ô∏è Hardware</p>\n",
    "    <ul>\n",
    "      <li><code>device</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; padding: 10px; background-color: rgba(200, 0, 200, 0.1); border-radius: 5px; border: 1px solid rgba(200, 0, 200, 0.2);\">\n",
    "    <p style=\"font-weight: bold\">‚ö° Performance</p>\n",
    "    <ul>\n",
    "      <li><code>max_calls_per_min</code></li>\n",
    "      <li><code>use_n_param</code></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "claimqa = LongTextQA(\n",
    "    llm=gemini_flash,\n",
    "    question_generator_llm=gemini_flash,\n",
    "    aggregation=\"mean\",  # switch to 'min' for more conservative scoring\n",
    "    response_refinement=True,  # whether to filter out low-confidence claims\n",
    "    scorers=[\"noncontradiction\"],\n",
    "    # max_calls_per_min=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### üîÑ Class Methods\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 100%; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "  <tr>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 25%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Method</th>\n",
    "    <th style=\"background-color: rgba(200, 200, 200, 0.2); width: 75%; padding: 8px; text-align: left; border: 1px solid rgba(127, 127, 127, 0.2);\">Description & Parameters</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BlackBoxUQ.generate_and_score</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Generate LLM responses, sampled LLM (candidate) responses, and compute confidence scores for the provided prompts.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>prompts</code> - (<strong>List[str] or List[List[BaseMessage]]</strong>) A list of input prompts for the model.</li>\n",
    "        <li><code>num_questions</code> - (<strong>int, default=2</strong>) Specifies how many questions to generate per claim/sentence.</li>\n",
    "        <li><code>num_claim_qa_responses</code> - (<strong>int, default=5</strong>) Specifies how many sampled responses to generate per claim/sentence question.</li>        \n",
    "        <li><code>response_refinement_threshold</code> - (<strong>float, default=1/3</strong>) Threshold for uncertainty-aware filtering. Claims with confidence scores below this threshold are dropped from the refined response. Only used if response_refinement is True.</li> \n",
    "        <li><code>show_progress_bars</code> - (<strong>bool, default=True</strong>) If True, displays a progress bar while generating and scoring responses.</li>        \n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (prompts, responses, sampled responses, and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Complete end-to-end uncertainty quantification when starting with prompts.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"font-weight: bold; vertical-align: top; padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">BlackBoxUQ.score</td>\n",
    "    <td style=\"padding: 8px; border: 1px solid rgba(127, 127, 127, 0.2);\">\n",
    "      <p>Compute confidence scores on provided LLM responses. Should only be used if responses and sampled responses are already generated.</p>\n",
    "      <p><strong>Parameters:</strong></p>\n",
    "      <ul>\n",
    "        <li><code>prompts</code> - (<strong>List[str]</strong>) A list of input prompts for the model.</li>\n",
    "        <li><code>responses</code> - (<strong>List[str]</strong>) A list of LLM responses for the prompts.</li>\n",
    "        <li><code>num_questions</code> - (<strong>int, default=2</strong>) Specifies how many questions to generate per claim/sentence.</li>\n",
    "        <li><code>num_claim_qa_responses</code> - (<strong>int, default=5</strong>) Specifies how many sampled responses to generate per claim/sentence question.</li>       \n",
    "        <li><code>response_refinement_threshold</code> - (<strong>float, default=1/3</strong>) Threshold for uncertainty-aware filtering. Claims with confidence scores below this threshold are dropped from the refined response. Only used if response_refinement is True.</li> \n",
    "        <li><code>show_progress_bars</code> - (<strong>bool, default=True</strong>) If True, displays a progress bar while scoring responses.</li>  \n",
    "      </ul>\n",
    "      <p><strong>Returns:</strong> <code>UQResult</code> containing data (responses, sampled responses, and confidence scores) and metadata</p>\n",
    "      <div style=\"background-color: rgba(0, 200, 0, 0.1); padding: 8px; border-radius: 3px; margin-top: 10px; border: 1px solid rgba(0, 200, 0, 0.2); margin-right: 5px; box-sizing: border-box; width: 100%;\">\n",
    "        <strong>üí° Best For:</strong> Computing uncertainty scores when responses are already generated elsewhere.\n",
    "      </div>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196a94cb761b44aa9e5ab557b4ed9c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = await claimqa.generate_and_score(prompts=factscore.prompt.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df = results.to_df()\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how the response refinement operates, let's view an example. We first view the fine-grained claim-level data, including the claims in the original response, the claim-level confidence scores, and whether each claim was removed during the response refinement process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View fine-grained claim data for a response\n",
    "result_df.claims_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine a particular claim in the response that was removed because its confidence score was too low. Let's see how this is reflected in the original vs. the refined response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"**Original response:**\\n {result_df.response[1]}\")\n",
    "print(\" \")\n",
    "print(f\"**Example claim to be removed:**\\n {result_df.claims_data[1][0]['claim']}\")\n",
    "print(\" \")\n",
    "print(f\"**Refined response:**\\n {result_df.refined_response[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Evaluate Hallucination Detection Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate hallucination detection performance, we 'grade' the atomic claims in the responses against an answer key. Here, we use UQLM's out-of-the-box `FactScoreGrader`, which can be used with [LangChain Chat Model](https://js.langchain.com/docs/integrations/chat/). **If you are using your own prompts/questions, be sure to update the grading method accordingly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up the LLM grader\n",
    "grader = FactScoreGrader(llm=gemini_flash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before grading, we need to have claims formatted in list of lists where each interior list corresponds to a generated response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert claims to list of lists\n",
    "claims_data_lists = claims_dicts_to_lists(result_df.claims_data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grade orinal responses against the answer key using the grader\n",
    "result_df[\"claim_grades\"] = await grader.grade_claims(claim_sets=claims_data_lists[\"claim\"], answers=factscore[\"wikipedia_text\"].to_list())\n",
    "result_df[\"answer\"] = factscore[\"wikipedia_text\"]\n",
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before grading, we need to have claims formatted in list of lists where each interior list corresponds to a generated response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_claim_scores, all_claim_grades = [], []\n",
    "for i in range(len(result_df)):\n",
    "    all_claim_scores.extend(claims_data_lists[\"noncontradiction\"][i])\n",
    "    all_claim_grades.extend(result_df[\"claim_grades\"][i])\n",
    "\n",
    "print(f\"\"\"Baseline LLM accuracy: {np.mean(all_claim_grades)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Claim-Level Hallucination Detection AUROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate fine-grained hallucination detection performance, we compute AUROC of claim-level hallucination detection. Below, we plot the ROC curve and report these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true=all_claim_grades, y_score=all_claim_scores)\n",
    "roc_auc = roc_auc_score(y_true=all_claim_grades, y_score=all_claim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df.to_parquet(\"/home/jupyter/gemini_flash_factscore_300_1q.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Gains from Uncertainty-Aware Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we evaluate the gains from uncertainty-aware decoding (UAD) by measuring the factual precision over claims at various filtering thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model_accuracies(scores=all_claim_scores, correct_indicators=all_claim_grades, title=\"LLM Accuracy by Claim Confidence Threshold\", display_percentage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we have selected a threshold of 0.35, we can measure LLM accuracy with and without UAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresh = 1 / 3\n",
    "filtered_grades, filtered_scores = [], []\n",
    "for grade, score in zip(all_claim_grades, all_claim_scores):\n",
    "    if score > thresh:\n",
    "        filtered_grades.append(grade)\n",
    "        filtered_scores.append(score)\n",
    "\n",
    "print(f\"Baseline LLM factual precision: {np.mean(all_claim_grades)}\")\n",
    "print(f\"UAD-Improved LLM factual precision: {np.mean(filtered_grades)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Scorer Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long-form uncertainty quantification implements a three-stage pipeline after response generation:\n",
    "\n",
    "1. Response Decomposition: The response $y$ is decomposed into units (claims or sentences), where a unit as denoted as $s$.\n",
    "\n",
    "2. Unit-Level Confidence Scoring: Confidence scores are computed using function $c_g(s;\\cdot) \\in [0, 1]$. Higher scores indicate greater likelihood of factual correctness. Units with scores below threshold $\\tau$ are flagged as potential hallucinations.\n",
    "\n",
    "3. Response-Level Aggregation: Unit scores are combined to provide an overall response confidence.\n",
    "\n",
    "The Claim-QA approach demonstrated here is adapted from [Farquhar et al., 2024](https://www.nature.com/articles/s41586-024-07421-0). It uses an LLM to convert each unit (sentence or claim) into a question for which that unit would be the answer. The method measures consistency across multiple responses to these questions, effectively applying standard black-box uncertainty quantification to those sampled responses to the unit questions. Formally, a claim-QA scorer $c_g(s;\\cdot)$ is defined as follows:\n",
    "\n",
    "$$c_g(s; y_0^{(s)}, \\mathbf{y}^{(s)}_{\\text{cand}}) = \\frac{1}{m} \\sum_{j=1}^m \\eta(y_0^{(s)}, y_j^{(s)})$$\n",
    "\n",
    "where $y_0^{(s)}$ is the original unit response, $\\mathbf{y}^{(s)}_{\\text{cand}} = {y_1^{(s)}, ..., y_m^{(s)}}$ are $m$ candidate responses to the unit's question, and $\\eta$ is a consistency function such as contradiction probability, cosine similarity, or BERTScore F1. Semantic entropy, which follows a slightly different functional form, can also be used to measure consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬© 2025 CVS Health and/or one of its affiliates. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "uqlm_my_test",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "uqlm_my_test",
   "language": "python",
   "name": "uqlm_my_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
